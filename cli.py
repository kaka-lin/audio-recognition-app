import argparse
from backends import get_backend


def run_cli(args):
    parser = argparse.ArgumentParser(description="Run Multi-ASR Toolkit in CLI mode")

    parser.add_argument("audio_path", help="è¼¸å…¥çš„éŸ³è¨Šæª”æ¡ˆè·¯å¾‘ï¼ˆæ”¯æ´ mp3, wav ç­‰ï¼‰")
    parser.add_argument("--backend", choices=["transformers", "faster-whisper", "whisper", "speech-recognition"],
                        default="transformers", help="é¸æ“‡è¾¨è­˜å¾Œç«¯")
    parser.add_argument("--language", default="auto", help="èªéŸ³èªè¨€ï¼ˆå¦‚ zh, en, ja, koï¼‰")
    parser.add_argument("--model-size", default="small", help="æ¨¡å‹å¤§å°ï¼Œä¾‹å¦‚ smallã€mediumã€large")

    args = parser.parse_args()

    # è¨­å®šæ¨¡å‹åç¨±ï¼ˆæŸäº› backend ä¸éœ€è¦ï¼‰
    if args.backend == "transformers":
        model_name = f"openai/whisper-{args.model_size}"
    elif args.backend == "speech-recognition":
        model_name = None
    else:
        model_name = args.model_size

    # æº–å‚™ backend
    kwargs = {"language": args.language}
    if model_name:
        kwargs["model_size"] = model_name

    asr = get_backend(args.backend, **kwargs)

    print(f"[INFO] ä½¿ç”¨ {args.backend} å¾Œç«¯é€²è¡Œè¾¨è­˜...")
    text = asr.transcribe(args.audio_path)
    print("\nğŸ“ è¾¨è­˜çµæœï¼š\n")
    print(text)
